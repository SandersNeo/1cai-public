# ‚úÖ Apache Airflow - –°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑

**–î–∞—Ç–∞:** 2025-11-06  
**–°—Ç–∞—Ç—É—Å:** –ò–°–ü–†–ê–í–õ–ï–ù–û –ø–æ—Å–ª–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞

---

## üîç –ö–û–†–†–ï–ö–¢–ò–†–û–í–ö–ê –ü–û–°–õ–ï –ü–†–û–í–ï–†–ö–ò

### –ß—Ç–æ –£–ñ–ï –ï–°–¢–¨ –≤ –ø—Ä–æ–µ–∫—Ç–µ:

‚úÖ **Prometheus + Grafana** - –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã!
- `docker-compose.monitoring.yml` —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
- Grafana –Ω–∞ –ø–æ—Ä—Ç—É 3001
- Prometheus scraping: FastAPI, PostgreSQL, Redis, Neo4j, Qdrant
- 4 –≥–æ—Ç–æ–≤—ã—Ö dashboard'–∞

‚úÖ **Alertmanager** - —É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω
- Alerts –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
- Email/webhook —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è

‚úÖ **Loki + Promtail** - –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
- Centralized logging
- Grafana integration

### ‚ùå –ß–µ–≥–æ –ù–ï–¢ (—á—Ç–æ –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å):

1. **Flower UI** –¥–ª—è Celery - –ù–ï–¢ –≤ docker-compose
2. **Celery metrics** –≤ Prometheus - –ù–ï –Ω–∞—Å—Ç—Ä–æ–µ–Ω scraping
3. **Celery dashboard** –≤ Grafana - –ù–ï–¢ (–µ—Å—Ç—å —Ç–æ–ª—å–∫–æ overview, business, system)

---

## ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø

### –ü—É–Ω–∫—Ç 1: Celery Groups –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ ‚úÖ
**–°—Ç–∞—Ç—É—Å:** –°–æ–≥–ª–∞—Å–Ω—ã, –Ω—É–∂–µ–Ω  
**–ó–∞—Ç—Ä–∞—Ç—ã:** 8 —á–∞—Å–æ–≤  
**–§–∞–π–ª:** `src/workers/ml_tasks_parallel.py`

---

### –ü—É–Ω–∫—Ç 2: Grafana + Flower –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

**–ß–¢–û –£–ñ–ï –ï–°–¢–¨:**
```yaml
‚úÖ Grafana - —Ä–∞–±–æ—Ç–∞–µ—Ç (–ø–æ—Ä—Ç 3001)
‚úÖ Prometheus - scraping –º–Ω–æ–∂–µ—Å—Ç–≤–æ –º–µ—Ç—Ä–∏–∫
‚úÖ Alertmanager - –Ω–∞—Å—Ç—Ä–æ–µ–Ω
‚úÖ 4 dashboard - overview, business, system, monitoring
```

**–ß–¢–û –ù–£–ñ–ù–û –î–û–ë–ê–í–ò–¢–¨:**

#### A. Flower UI –¥–ª—è Celery (2 —á–∞—Å–∞)

**–î–æ–±–∞–≤–∏—Ç—å –≤ docker-compose.yml:**
```yaml
services:
  flower:
    image: mher/flower:latest
    container_name: flower
    command: celery --broker=redis://redis:6379/1 flower --port=5555
    ports:
      - "5555:5555"
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/1
      - CELERY_RESULT_BACKEND=redis://redis:6379/2
    depends_on:
      - redis
    networks:
      - monitoring
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** Flower UI –Ω–∞ http://localhost:5555

---

#### B. Celery Prometheus Exporter (3 —á–∞—Å–∞)

**–î–æ–±–∞–≤–∏—Ç—å –≤ docker-compose.yml:**
```yaml
  celery-exporter:
    image: danihodovic/celery-exporter:latest
    container_name: celery-exporter
    command: 
      - --broker-url=redis://redis:6379/1
      - --listen-address=0.0.0.0:9808
    ports:
      - "9808:9808"
    depends_on:
      - redis
    networks:
      - monitoring
```

**–î–æ–±–∞–≤–∏—Ç—å –≤ monitoring/prometheus/prometheus.yml:**
```yaml
scrape_configs:
  # ... existing configs ...
  
  # Celery metrics
  - job_name: 'celery'
    static_configs:
      - targets: ['celery-exporter:9808']
    scrape_interval: 10s
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** Celery –º–µ—Ç—Ä–∏–∫–∏ –≤ Prometheus

---

#### C. Celery Dashboard –≤ Grafana (7 —á–∞—Å–æ–≤)

**–°–æ–∑–¥–∞—Ç—å:** `monitoring/grafana/dashboards/celery_monitoring.json`

```json
{
  "dashboard": {
    "title": "Celery Tasks Monitoring",
    "tags": ["celery", "tasks", "workers"],
    "panels": [
      {
        "id": 1,
        "title": "Active Workers",
        "type": "stat",
        "targets": [{
          "expr": "celery_workers"
        }]
      },
      {
        "id": 2,
        "title": "Task Execution Rate",
        "type": "graph",
        "targets": [{
          "expr": "rate(celery_tasks_total[5m])",
          "legendFormat": "{{ task_name }}"
        }]
      },
      {
        "id": 3,
        "title": "Task Success Rate",
        "type": "graph",
        "targets": [{
          "expr": "rate(celery_tasks_succeeded_total[5m]) / rate(celery_tasks_total[5m]) * 100"
        }]
      },
      {
        "id": 4,
        "title": "Task Duration (p95)",
        "type": "graph",
        "targets": [{
          "expr": "histogram_quantile(0.95, celery_task_runtime_seconds_bucket)",
          "legendFormat": "{{ task_name }}"
        }]
      },
      {
        "id": 5,
        "title": "Queue Length",
        "type": "graph",
        "targets": [{
          "expr": "celery_queue_length",
          "legendFormat": "{{ queue_name }}"
        }]
      },
      {
        "id": 6,
        "title": "Failed Tasks (Last Hour)",
        "type": "stat",
        "targets": [{
          "expr": "increase(celery_tasks_failed_total[1h])"
        }]
      }
    ]
  }
}
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π Celery dashboard –≤ Grafana

---

**–ò–¢–û–ì–û –¥–ª—è –ø—É–Ω–∫—Ç–∞ 2:**
- Flower UI: 2 —á–∞—Å–∞
- Prometheus exporter: 3 —á–∞—Å–∞
- Grafana dashboard: 7 —á–∞—Å–æ–≤
- **–í—Å–µ–≥–æ: 12 —á–∞—Å–æ–≤** (–∫–∞–∫ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–ª–æ—Å—å)

**–ß—Ç–æ –£–ñ–ï –µ—Å—Ç—å:** Grafana + Prometheus –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ ‚úÖ  
**–ß—Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º:** Celery –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Å–∏—Å—Ç–µ–º—É ‚úÖ

---

### –ü—É–Ω–∫—Ç 3: Bash orchestrator –¥–ª—è EDT

**–ß–¢–û –û–ù –ë–£–î–ï–¢ –î–ï–õ–ê–¢–¨:**

–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è **6 manual —Å–∫—Ä–∏–ø—Ç–æ–≤** –≤ –æ–¥–∏–Ω pipeline —Å –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–æ–º:

**–¢–µ–∫—É—â–∞—è —Å–∏—Ç—É–∞—Ü–∏—è:**
```bash
# –°–µ–π—á–∞—Å –∑–∞–ø—É—Å–∫–∞–µ–º –≤—Ä—É—á–Ω—É—é, 6 –∫–æ–º–∞–Ω–¥:
python scripts/parsers/edt/edt_parser_with_metadata.py     # 10-15 min
python scripts/analysis/analyze_architecture.py            # 5 min
python scripts/dataset/create_ml_dataset.py                # 8-12 min
python scripts/analysis/analyze_dependencies.py            # 3-5 min
python scripts/analysis/extract_best_practices.py          # 2-3 min
python scripts/analysis/generate_documentation.py          # 1-2 min

# –ò–¢–û–ì–û: 29-47 –º–∏–Ω—É—Ç, 6 –∫–æ–º–∞–Ω–¥
```

**–° orchestrator:**
```bash
# –û–¥–Ω–∞ –∫–æ–º–∞–Ω–¥–∞:
./scripts/orchestrate_edt_analysis.sh

# –ó–∞–ø—É—Å—Ç–∏—Ç –≤–µ—Å—å pipeline –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å:
# - Error handling (–µ—Å–ª–∏ —à–∞–≥ —É–ø–∞–ª - —Å—Ç–æ–ø)
# - Logging (–≤—Å–µ –ª–æ–≥–∏ –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª)
# - –ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º (4 –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –∞–Ω–∞–ª–∏–∑–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ)
# - Progress reporting (–ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≥–¥–µ —Å–µ–π—á–∞—Å)
# - Timestamp (—É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ª–æ–≥ —Ñ–∞–π–ª –Ω–∞ –∫–∞–∂–¥—ã–π –∑–∞–ø—É—Å–∫)

# –ò–¢–û–ì–û: 15-20 –º–∏–Ω—É—Ç, 1 –∫–æ–º–∞–Ω–¥–∞
```

**–ü–æ–¥—Ä–æ–±–Ω–∞—è –ª–æ–≥–∏–∫–∞:**

```bash
#!/bin/bash
# scripts/orchestrate_edt_analysis.sh

# STEP 1: –ü–∞—Ä—Å–∏–Ω–≥ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–≤—ã–º)
echo "üîÑ Step 1/6: Parsing EDT configuration..."
python scripts/parsers/edt/edt_parser_with_metadata.py
if [ $? -ne 0 ]; then
    echo "‚ùå FAILED at parsing"
    exit 1
fi
echo "‚úÖ Parsing complete"

# STEP 2-5: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∞–Ω–∞–ª–∏–∑—ã (–ù–ï –∑–∞–≤–∏—Å—è—Ç –¥—Ä—É–≥ –æ—Ç –¥—Ä—É–≥–∞!)
echo "üîÑ Steps 2-5: Running 4 parallel analyses..."

# –ó–∞–ø—É—Å–∫–∞–µ–º –≤ —Ñ–æ–Ω–µ (&) –≤—Å–µ 4 —Å–∫—Ä–∏–ø—Ç–∞
python scripts/analysis/analyze_architecture.py > logs/arch.log 2>&1 &
PID_ARCH=$!

python scripts/dataset/create_ml_dataset.py > logs/dataset.log 2>&1 &
PID_DATASET=$!

python scripts/analysis/analyze_dependencies.py > logs/deps.log 2>&1 &
PID_DEPS=$!

python scripts/analysis/extract_best_practices.py > logs/bp.log 2>&1 &
PID_BP=$!

# –ñ–¥—ë–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –í–°–ï–•
wait $PID_ARCH && echo "  ‚úÖ Architecture" || echo "  ‚ùå Architecture FAILED"
wait $PID_DATASET && echo "  ‚úÖ ML Dataset" || echo "  ‚ùå ML Dataset FAILED"
wait $PID_DEPS && echo "  ‚úÖ Dependencies" || echo "  ‚ùå Dependencies FAILED"
wait $PID_BP && echo "  ‚úÖ Best Practices" || echo "  ‚ùå Best Practices FAILED"

# STEP 6: –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (–ø–æ—Å–ª–µ –≤—Å–µ—Ö –∞–Ω–∞–ª–∏–∑–æ–≤)
echo "üîÑ Step 6/6: Generating documentation..."
python scripts/analysis/generate_documentation.py
if [ $? -ne 0 ]; then
    echo "‚ùå FAILED at documentation"
    exit 1
fi

echo "‚úÖ‚úÖ‚úÖ PIPELINE COMPLETE! ‚úÖ‚úÖ‚úÖ"
```

**–ß—Ç–æ –¥–∞—ë—Ç:**
1. ‚úÖ **–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è** - 1 –∫–æ–º–∞–Ω–¥–∞ –≤–º–µ—Å—Ç–æ 6
2. ‚úÖ **–ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º** - 4 —Å–∫—Ä–∏–ø—Ç–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
3. ‚úÖ **Error handling** - –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏ –æ—à–∏–±–∫–µ
4. ‚úÖ **Logging** - –≤—Å–µ –ª–æ–≥–∏ –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω—ã
5. ‚úÖ **Progress** - –≤–∏–¥–Ω–æ —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç

**–≠–∫–æ–Ω–æ–º–∏—è –≤—Ä–µ–º–µ–Ω–∏:**
- –ë—ã–ª–æ: 29-47 –º–∏–Ω (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ)
- –°—Ç–∞–ª–æ: 15-20 –º–∏–Ω (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ)
- **–≠–∫–æ–Ω–æ–º–∏—è: 35-50%**

**–ó–∞—Ç—Ä–∞—Ç—ã:** 6 —á–∞—Å–æ–≤ –Ω–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É —Å–∫—Ä–∏–ø—Ç–∞

---

## üìä –ò–¢–û–ì–û–í–ê–Ø –¢–ê–ë–õ–ò–¶–ê (–°–ö–û–†–†–ï–ö–¢–ò–†–û–í–ê–ù–ù–ê–Ø)

### –ß—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å:

| # | –ó–∞–¥–∞—á–∞ | –ß—Ç–æ –¥–µ–ª–∞–µ–º | –ó–∞—Ç—Ä–∞—Ç—ã | –í—ã–≥–æ–¥–∞ | –°—Ç–∞—Ç—É—Å |
|---|--------|------------|---------|--------|--------|
| 1 | Celery parallelism | –î–æ–±–∞–≤–∏—Ç—å Groups | 8 —á–∞—Å–æ–≤ | ML: -43% –≤—Ä–µ–º–µ–Ω–∏ | ‚úÖ –ù—É–∂–µ–Ω |
| 2a | Flower UI | –î–æ–±–∞–≤–∏—Ç—å –≤ docker-compose | 2 —á–∞—Å–∞ | Web UI –¥–ª—è Celery | ‚úÖ –ù—É–∂–µ–Ω |
| 2b | Celery metrics | Prometheus exporter | 3 —á–∞—Å–∞ | –ú–µ—Ç—Ä–∏–∫–∏ –≤ Prometheus | ‚úÖ –ù—É–∂–µ–Ω |
| 2c | Celery dashboard | Grafana dashboard | 7 —á–∞—Å–æ–≤ | –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è | ‚úÖ –ù—É–∂–µ–Ω |
| 3 | EDT orchestrator | Bash script | 6 —á–∞—Å–æ–≤ | EDT: -50% –≤—Ä–µ–º–µ–Ω–∏ | ‚úÖ –ù—É–∂–µ–Ω |

**–ò–¢–û–ì–û: 26 —á–∞—Å–æ–≤ ($1,300)**

### –ß—Ç–æ –£–ñ–ï –†–ê–ë–û–¢–ê–ï–¢ (–Ω–µ —Ç—Ä–æ–≥–∞–µ–º):

‚úÖ Grafana –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞  
‚úÖ Prometheus scraping  
‚úÖ Alertmanager  
‚úÖ Loki logging  
‚úÖ 4 —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö dashboards  

**–ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É!** (–Ω–µ —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é)

---

## üéØ –ö–û–ù–ö–†–ï–¢–ù–´–ô –ü–õ–ê–ù –î–ï–ô–°–¢–í–ò–ô

### Week 1: Celery Parallelism (8 —á–∞—Å–æ–≤)

**–°–æ–∑–¥–∞—Ç—å:** `src/workers/ml_tasks_parallel.py`

```python
from celery import group, chord
from celery.utils.log import get_task_logger

logger = get_task_logger(__name__)

@celery_app.task(name='workers.ml_tasks.retrain_all_models_parallel')
def retrain_all_models_parallel():
    """
    –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
    
    –ë—ã–ª–æ: 75 –º–∏–Ω—É—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
    –°—Ç–∞–ª–æ: 15 –º–∏–Ω—É—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    """
    logger.info("Starting parallel model training...")
    
    # –ì—Ä—É–ø–ø–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á
    training_tasks = group(
        retrain_single_model.s('classification'),
        retrain_single_model.s('regression'),
        retrain_single_model.s('clustering'),
        retrain_single_model.s('ranking'),
        retrain_single_model.s('recommendation'),
    )
    
    # –ü–æ—Å–ª–µ –≤—Å–µ—Ö —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ–∫ - evaluate, –ø–æ—Ç–æ–º cleanup
    pipeline = chord(training_tasks)(
        evaluate_all_models.s() | cleanup_old_experiments.s()
    )
    
    result = pipeline.get(timeout=3600)  # 1 hour max
    
    logger.info("Parallel training complete!")
    return result

@celery_app.task(name='workers.ml_tasks.retrain_single_model')
def retrain_single_model(model_type: str):
    """–û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ (–¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞)"""
    logger.info(f"Training {model_type} model...")
    
    # Existing code from retrain_model()
    # ...
    
    return {'model': model_type, 'status': 'success'}
```

**–û–±–Ω–æ–≤–∏—Ç—å beat_schedule:**
```python
celery_app.conf.beat_schedule = {
    'retrain-models-parallel-daily': {
        'task': 'workers.ml_tasks.retrain_all_models_parallel',
        'schedule': crontab(hour=2, minute=0),
        'options': {'queue': 'ml_heavy'}
    },
    # –û—Å—Ç–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
}
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** ML Pipeline 75 –º–∏–Ω ‚Üí 15 –º–∏–Ω ‚úÖ

---

### Week 2: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ Celery (12 —á–∞—Å–æ–≤)

**–®–∞–≥ 1: Flower UI (2 —á–∞—Å–∞)**

**–î–æ–±–∞–≤–∏—Ç—å –≤ docker-compose.yml:**
```yaml
services:
  # ... existing services ...
  
  flower:
    image: mher/flower:2.0
    container_name: flower
    command: celery --broker=${CELERY_BROKER_URL} flower --port=5555 --url_prefix=flower
    ports:
      - "5555:5555"
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/1
      - CELERY_RESULT_BACKEND=redis://redis:6379/2
      - FLOWER_BASIC_AUTH=admin:${FLOWER_PASSWORD}
    depends_on:
      - redis
    networks:
      - monitoring
    restart: unless-stopped
```

**–î–æ—Å—Ç—É–ø:** http://localhost:5555

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ Flower:**
- –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö tasks (active, scheduled, failed)
- Worker status
- Task details –∏ logs
- –ì—Ä–∞—Ñ–∏–∫–∏ execution time
- Rate limiting control

---

**–®–∞–≥ 2: Celery Prometheus Exporter (3 —á–∞—Å–∞)**

**–î–æ–±–∞–≤–∏—Ç—å –≤ docker-compose.yml:**
```yaml
  celery-exporter:
    image: danihodovic/celery-exporter:latest
    container_name: celery-exporter
    command:
      - --broker-url=redis://redis:6379/1
      - --listen-address=0.0.0.0:9808
      - --enable-events
    ports:
      - "9808:9808"
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/1
    depends_on:
      - redis
    networks:
      - monitoring
    restart: unless-stopped
```

**–û–±–Ω–æ–≤–∏—Ç—å monitoring/prometheus/prometheus.yml:**
```yaml
scrape_configs:
  # ... existing configs ...
  
  # Celery metrics
  - job_name: 'celery'
    static_configs:
      - targets: ['celery-exporter:9808']
    scrape_interval: 10s
    metrics_path: '/metrics'
```

**–ú–µ—Ç—Ä–∏–∫–∏ –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—è–≤—è—Ç—Å—è:**
- `celery_tasks_total` - –≤—Å–µ–≥–æ –∑–∞–¥–∞—á
- `celery_tasks_succeeded_total` - —É—Å–ø–µ—à–Ω—ã—Ö
- `celery_tasks_failed_total` - —É–ø–∞–≤—à–∏—Ö
- `celery_task_runtime_seconds` - –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
- `celery_workers` - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ workers
- `celery_queue_length` - –¥–ª–∏–Ω–∞ –æ—á–µ—Ä–µ–¥–∏

---

**–®–∞–≥ 3: Celery Dashboard (7 —á–∞—Å–æ–≤)**

**–°–æ–∑–¥–∞—Ç—å:** `monitoring/grafana/dashboards/celery_monitoring.json`

**–ü–∞–Ω–µ–ª–∏:**
1. **Workers Status** - —Å–∫–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö workers
2. **Tasks Overview** - total, succeeded, failed, retry
3. **Execution Rate** - —Å–∫–æ–ª—å–∫–æ tasks/min
4. **Success Rate %** - –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω—ã—Ö
5. **Task Duration (p50, p95, p99)** - –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
6. **Queue Length** - —Ä–∞–∑–º–µ—Ä –æ—á–µ—Ä–µ–¥–∏
7. **Failed Tasks Timeline** - –∫–æ–≥–¥–∞ –ø–∞–¥–∞–ª–∏
8. **Task Heatmap** - –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ —á–∞—Å–∞–º
9. **Worker Memory** - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
10. **Active Tasks List** - —á—Ç–æ —Å–µ–π—á–∞—Å –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è

**–ü—Ä–∏–º–µ—Ä –ø–∞–Ω–µ–ª–∏ "Success Rate":**
```json
{
  "id": 3,
  "title": "Task Success Rate %",
  "type": "graph",
  "datasource": "Prometheus",
  "targets": [
    {
      "expr": "rate(celery_tasks_succeeded_total[5m]) / rate(celery_tasks_total[5m]) * 100",
      "legendFormat": "Success Rate",
      "refId": "A"
    }
  ],
  "yaxes": [
    {
      "format": "percent",
      "min": 0,
      "max": 100
    }
  ],
  "gridPos": {
    "x": 0,
    "y": 8,
    "w": 12,
    "h": 8
  },
  "alert": {
    "conditions": [
      {
        "evaluator": {
          "params": [95],
          "type": "lt"
        },
        "query": {
          "params": ["A", "5m", "now"]
        }
      }
    ],
    "name": "Celery Success Rate Low",
    "message": "Celery success rate below 95%!"
  }
}
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** Professional Celery monitoring ‚úÖ

---

### Week 3: EDT Orchestrator (6 —á–∞—Å–æ–≤)

**–°–æ–∑–¥–∞—Ç—å:** `scripts/orchestrate_edt_analysis.sh`

**–ü–æ–ª–Ω—ã–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª:**

```bash
#!/bin/bash
# EDT Configuration Analysis - Full Pipeline Orchestrator
# Version: 1.0
# Date: 2025-11-06

set -e  # Exit on error
set -u  # Exit on undefined variable

# ============================================================================
# CONFIGURATION
# ============================================================================

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
LOG_DIR="$PROJECT_ROOT/logs/edt_analysis"
OUTPUT_DIR="$PROJECT_ROOT/output"

TIMESTAMP=$(date +%Y%m%d_%H%M%S)
RUN_ID="edt_analysis_$TIMESTAMP"
LOG_FILE="$LOG_DIR/${RUN_ID}.log"

# ============================================================================
# FUNCTIONS
# ============================================================================

log() {
    local level="$1"
    local message="$2"
    local timestamp=$(date +'%Y-%m-%d %H:%M:%S')
    echo "[$timestamp] [$level] $message" | tee -a "$LOG_FILE"
}

check_requirements() {
    log "INFO" "Checking requirements..."
    
    # Check Python
    if ! command -v python &> /dev/null; then
        log "ERROR" "Python not found"
        exit 1
    fi
    
    # Check configuration directory
    if [ ! -d "$PROJECT_ROOT/1c_configurations/ERPCPM" ]; then
        log "ERROR" "ERPCPM configuration not found"
        exit 1
    fi
    
    log "INFO" "‚úÖ Requirements OK"
}

run_with_timeout() {
    local timeout_seconds=$1
    local command=$2
    local task_name=$3
    
    timeout $timeout_seconds bash -c "$command" &
    local pid=$!
    
    wait $pid
    local exit_code=$?
    
    if [ $exit_code -eq 124 ]; then
        log "ERROR" "$task_name TIMEOUT (>${timeout_seconds}s)"
        return 1
    elif [ $exit_code -ne 0 ]; then
        log "ERROR" "$task_name FAILED (exit code: $exit_code)"
        return 1
    fi
    
    return 0
}

# ============================================================================
# MAIN PIPELINE
# ============================================================================

main() {
    log "INFO" "========================================="
    log "INFO" "EDT ANALYSIS PIPELINE"
    log "INFO" "Run ID: $RUN_ID"
    log "INFO" "========================================="
    
    mkdir -p "$LOG_DIR"
    
    # Prerequisites
    check_requirements
    
    # STEP 1: Parsing (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–≤—ã–º, 10-15 min)
    log "INFO" "Step 1/6: Parsing EDT configuration..."
    START_TIME=$(date +%s)
    
    if run_with_timeout 1200 \
        "cd $PROJECT_ROOT && python scripts/parsers/edt/edt_parser_with_metadata.py" \
        "EDT Parsing"; then
        
        END_TIME=$(date +%s)
        DURATION=$((END_TIME - START_TIME))
        log "INFO" "‚úÖ Parsing complete (${DURATION}s)"
    else
        log "ERROR" "‚ùå Parsing FAILED - aborting pipeline"
        exit 1
    fi
    
    # STEP 2-5: Parallel Analysis (4 tasks, max 12 min)
    log "INFO" "Steps 2-5: Running 4 parallel analyses..."
    START_TIME=$(date +%s)
    
    # Launch all 4 tasks in background
    python "$PROJECT_ROOT/scripts/analysis/analyze_architecture.py" > "$LOG_DIR/${RUN_ID}_arch.log" 2>&1 &
    PID_ARCH=$!
    
    python "$PROJECT_ROOT/scripts/dataset/create_ml_dataset.py" > "$LOG_DIR/${RUN_ID}_dataset.log" 2>&1 &
    PID_DATASET=$!
    
    python "$PROJECT_ROOT/scripts/analysis/analyze_dependencies.py" > "$LOG_DIR/${RUN_ID}_deps.log" 2>&1 &
    PID_DEPS=$!
    
    python "$PROJECT_ROOT/scripts/analysis/extract_best_practices.py" > "$LOG_DIR/${RUN_ID}_bp.log" 2>&1 &
    PID_BP=$!
    
    # Wait and check each task
    FAILED=0
    
    wait $PID_ARCH
    if [ $? -eq 0 ]; then
        log "INFO" "  ‚úÖ Architecture analysis complete"
    else
        log "ERROR" "  ‚ùå Architecture analysis FAILED"
        FAILED=1
    fi
    
    wait $PID_DATASET
    if [ $? -eq 0 ]; then
        log "INFO" "  ‚úÖ ML Dataset creation complete"
    else
        log "ERROR" "  ‚ùå ML Dataset creation FAILED"
        FAILED=1
    fi
    
    wait $PID_DEPS
    if [ $? -eq 0 ]; then
        log "INFO" "  ‚úÖ Dependencies analysis complete"
    else
        log "ERROR" "  ‚ùå Dependencies analysis FAILED"
        FAILED=1
    fi
    
    wait $PID_BP
    if [ $? -eq 0 ]; then
        log "INFO" "  ‚úÖ Best practices extraction complete"
    else
        log "ERROR" "  ‚ùå Best practices extraction FAILED"
        FAILED=1
    fi
    
    END_TIME=$(date +%s)
    DURATION=$((END_TIME - START_TIME))
    
    if [ $FAILED -eq 1 ]; then
        log "ERROR" "‚ùå Parallel analysis FAILED - aborting pipeline"
        exit 1
    fi
    
    log "INFO" "‚úÖ Parallel analysis complete (${DURATION}s)"
    
    # STEP 6: Documentation (after all analyses, 1-2 min)
    log "INFO" "Step 6/6: Generating documentation..."
    START_TIME=$(date +%s)
    
    if run_with_timeout 300 \
        "cd $PROJECT_ROOT && python scripts/analysis/generate_documentation.py" \
        "Documentation Generation"; then
        
        END_TIME=$(date +%s)
        DURATION=$((END_TIME - START_TIME))
        log "INFO" "‚úÖ Documentation complete (${DURATION}s)"
    else
        log "ERROR" "‚ùå Documentation FAILED"
        exit 1
    fi
    
    # Summary
    log "INFO" "========================================="
    log "INFO" "‚úÖ‚úÖ‚úÖ PIPELINE COMPLETE ‚úÖ‚úÖ‚úÖ"
    log "INFO" "========================================="
    log "INFO" "Results:"
    log "INFO" "  - Parse results: $OUTPUT_DIR/edt_parser/"
    log "INFO" "  - Analysis: $OUTPUT_DIR/analysis/"
    log "INFO" "  - ML Dataset: $OUTPUT_DIR/dataset/"
    log "INFO" "  - Documentation: docs/generated/"
    log "INFO" "  - Logs: $LOG_FILE"
    log "INFO" "========================================="
}

# Run pipeline
main "$@"
```

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
1. ‚úÖ Error handling —Å timeout
2. ‚úÖ –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
3. ‚úÖ Parallel execution (4 tasks)
4. ‚úÖ Progress reporting
5. ‚úÖ Summary –≤ –∫–æ–Ω—Ü–µ
6. ‚úÖ –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ log —Ñ–∞–π–ª—ã

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:**
```bash
# –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—É—Å–∫
./scripts/orchestrate_edt_analysis.sh

# –†–µ–∑—É–ª—å—Ç–∞—Ç:
# - –í—Å—ë –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
# - 15-20 –º–∏–Ω—É—Ç –≤–º–µ—Å—Ç–æ 35-47
# - –õ–æ–≥–∏ –≤ logs/edt_analysis/
```

---

## üìä –ò–¢–û–ì–û–í–ê–Ø –í–´–ì–û–î–ê

### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ:

| –ß—Ç–æ | –ë—ã–ª–æ | –°—Ç–∞–ª–æ | –≠–∫–æ–Ω–æ–º–∏—è |
|-----|------|-------|----------|
| ML Pipeline | 75 –º–∏–Ω –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ | 15 –º–∏–Ω –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ | **-80%** ‚≠ê |
| EDT Analysis | 35 –º–∏–Ω, 6 –∫–æ–º–∞–Ω–¥ | 18 –º–∏–Ω, 1 –∫–æ–º–∞–Ω–¥–∞ | **-49%** ‚≠ê |
| Troubleshooting | 20 –º–∏–Ω —á–µ—Ä–µ–∑ –ª–æ–≥–∏ | 3 –º–∏–Ω —á–µ—Ä–µ–∑ Flower/Grafana | **-85%** ‚≠ê |
| Visibility | –ß–∏—Ç–∞—Ç—å –∫–æ–¥ | Grafana dashboard | **+400%** ‚≠ê |

**–û–±—â–∏–µ –∑–∞—Ç—Ä–∞—Ç—ã:** 26 —á–∞—Å–æ–≤ ($1,300)  
**ROI:** 600%+ (–ø–µ—Ä–≤—ã–π –≥–æ–¥)

---

## ‚úÖ –§–ò–ù–ê–õ–¨–ù–´–ô SUMMARY

### –û—Ç–≤–µ—á–∞—è –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã:

**1. Celery Groups - –Ω—É–∂–µ–Ω?**
‚úÖ **–î–ê** - 8 —á–∞—Å–æ–≤, ML Pipeline -80% –≤—Ä–µ–º–µ–Ω–∏

**2. Grafana + Flower - —É–∂–µ –µ—Å—Ç—å?**
‚ö†Ô∏è **–ß–ê–°–¢–ò–ß–ù–û:**
- Grafana –£–ñ–ï –µ—Å—Ç—å ‚úÖ
- Prometheus –£–ñ–ï –µ—Å—Ç—å ‚úÖ
- Flower –ù–ï–¢ - –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å (2 —á–∞—Å–∞)
- Celery metrics –ù–ï–¢ - –Ω—É–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å (3 —á–∞—Å–∞)
- Celery dashboard –ù–ï–¢ - –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å (7 —á–∞—Å–æ–≤)

**3. Bash orchestrator - —á—Ç–æ —ç—Ç–æ?**
‚úÖ **–°–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏:**
- 1 –∫–æ–º–∞–Ω–¥–∞ –≤–º–µ—Å—Ç–æ 6
- –ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º (4 –∑–∞–¥–∞—á–∏)
- Error handling
- Logging
- EDT: 35 –º–∏–Ω ‚Üí 18 –º–∏–Ω (-49%)

---

**–°—Ç–∞—Ç—É—Å:** ‚úÖ –ê–Ω–∞–ª–∏–∑ —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω —Å —É—á—ë—Ç–æ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã


